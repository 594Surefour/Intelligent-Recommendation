# <center>智能推荐系统期末作业

## 						   ——**Bandit Algorithm Plus In Recommendation**







<img src="Final.assets/截屏2021-06-14 下午6.23.37.png" alt="截屏2021-06-14 下午6.23.37" style="zoom: 25%;" />









### 学号：10172100255、10185102142

### 姓名：龙旷飞、李泽浩

### 指导老师：张伟

### 项目名称：Bandit Algorithm Plus In Recommendation

### 时间：2021年6月5日



<div style="page-break-after:always"></div>



## <center>目录<center>

### 1.[推荐系统中的新内容](####1、推荐算法简介)



### 2.[Bandit 算法](####2、基于用户的协同过滤算法UCF)



### 3.[改进LinUCB](####3、代码详解)



### 4.[结果分析及准确率](####4、结果分析及准确率)



### 5.[提交文件](####5、提交文件列表)



<div style="page-break-after: always;"></div>

## 1、推荐系统中的新内容

​		随着今天越来越多的直接面向消费者（DTC）平台的选择，大多数消费者无法订阅所有平台。订阅/购买决定是由内容（一个平台有哪些节目/电影）和用户体验（一个平台有多容易使用）共同驱动的。今天的消费者在考虑、购买和接触内容时，期望获得实时、精心策划的体验。无论是提高点击率、增加观看次数、观看时间、订阅或购买优质内容，媒体公司都在努力寻找方法，以提供更好的客户体验并扩大盈利能力。
​		推荐系统是实现这些目标的一个重要工具。DTC平台提供的推荐可以最大限度地发挥深度内容目录的价值，在消费者观看了最初将他们带到平台的内容之后，还可以保持他们的参与。例如，视频点播（VOD）平台的良好推荐可以通过在基于消费者行为的推荐中浮现长尾内容而增加收入。
​		我们首先回顾目前使用的常见的推荐系统的种类。然后，我们深入研究了这个领域中一些最令人兴奋的最新发展。

（1）常见的传统推荐系统
		常见系统可以被归类为基于内容的过滤或协作式过滤。基于内容的过滤是最简单的系统之一，但有时仍然是有用的。它是基于明确或隐含地提供的已知的用户偏好，以及关于项目特征的数据（如项目所属的类别）。虽然这些系统很容易实现，但它们的建议往往让人感觉是静态的，而且很难处理那些偏好未知的新用户。
		协同过滤是基于（用户、项目、评级）图元的。因此，与基于内容的过滤不同，它利用了其他用户的经验。亚马逊公司是这种方法的先驱，并发表了一篇早期的论文，后来获得了电气和电子工程师协会（IEEE）颁发的最经得起 "时间考验 "的论文。协同过滤的主要概念是，具有相似品味的用户（基于观察到的用户与物品的互动）更有可能与他们以前没有见过的物品产生相似的互动。
		与基于内容的过滤相比，协同过滤为多样性（推荐项目的不同程度）、偶然性（衡量成功或相关推荐的惊讶程度）和新颖性（推荐项目对用户的未知程度）提供了更好的结果。然而，协同过滤的计算成本较高，实施和管理起来也更加复杂和昂贵。尽管一些用于协同过滤的算法，如因子化机，比其他算法更轻便。协同过滤也有一个冷启动问题，因为如果没有大量的交互数据来训练模型，它很难推荐新的项目。
		除了这两类 "经典 "的推荐系统外，各种神经网络架构在推荐系统中也很常见。 有些实现了一种协作过滤的形式。其他的推荐系统则扩展到处理时间性数据，以便根据反映用户兴趣演变的用户行为序列来进行推荐。这些系统最初是基于各种递归神经网络（RNN）的。现在，它们利用基于Transformer的模型，通过自我关注来学习用户行为序列中的项目之间的依赖关系。
		与非深度学习模型如分解机相比，神经网通常是数据和计算密集型的，尽管这两种模型都在继续使用。例如，Amazon SageMaker是一个可管理的机器学习服务，支持从数据标签和处理到模型部署的整个项目生命周期，包括因子化机和Object2Vec的内置算法，这是一种可用于推荐系统的神经嵌入算法。

（2）新方法
		在过去的几年里，研究人员已经尝试了许多新的推荐系统方法。事实上，有这么多的方法，我们不能在此一一介绍。其中，混合系统越来越受欢迎。其中一些较新的方法并不相互排斥，可以与彼此或早期的技术相结合。一个例子是亚马逊Personalize，一个完全管理的个性化推荐服务。在Amazon Personalize中，用户个性化的首选算法（"配方"）结合了较新的基于bandit的方法和基于AWS最近论文的Hierarchical RNN。Bandit也是我们本次期末项目的研究方向。
		一个活跃的研究领域是纳入基于匪徒方法的推荐系统。 匪徒算法是强化学习（RL）的一种形式，试图在探索新的可能性和利用已经发现的有利可图的可能性之间取得平衡。 它们经常被用作静态A/B测试的替代品：一个关键的优势是它们能够实时适应。 这可以帮助克服冷启动的问题。
		在推荐系统的背景下，匪徒算法现在有很多应用，并已被整合到生产级系统中，如Amazon Personalize，它有效地将RNN与匪徒相结合，提供更准确的用户建模（高相关性）和有效的探索。 事实上，匪徒算法可以用来根据用户对每个系统提供的不同建议的反应，在几个推荐系统之间进行实时选择。
		匪徒的一个越来越重要的应用是在考虑到与用户满意度相关的多个目标和指标的系统中，和/或多个利益相关者（一个 "市场"--用户、广告商、平台持有人、内容所有者等）。例如，在一个音乐内容推荐系统中，一个额外的目标可能是为长尾艺术家和内容提供 "公平"，确保他们至少得到一些推荐。这种方法已经被Spotify等内容提供商研究过了，Spotify的一位研究人员在一个有趣的、可公开的演讲中讨论过。
		在AWS上，有多种方法可以使用基于bandit的系统。正如前一段所提到的，Amazon Personalize提供了一个完全管理的选项来做到这一点。一个管理较少的选择是使用Amazon SageMaker RL，它包括预建的RL库和算法，使强化学习容易上手。Amazon SageMaker RL中的contextual bandits算法可用于通过学习用户的反应来进行推荐，如点击推荐或不点击。在相关的文章中有一个笔记本的样本。

<div style="page-break-after:always"></div>

## 2、Bandit 算法

<img src="Final.assets/截屏2021-05-29 下午1.56.23.png" alt="截屏2021-05-29 下午1.56.23" style="zoom:33%;" />



#### 2.1 Bandit 起源

​		Bandit算法来源于历史悠久的赌博学，它要解决的问题是这样的：一个以利润最大化为目标的风险投资公司，该公司面临着一个两难的选择。何时投资于已经成功的公司，何时投资于尚未成功但有巨大潜力的公司。
投资学告诉我们：回报总是伴随着风险。一个成功的风险投资家必须处理好这种探索和开发的权衡：过多的探索意味着无法获得更高的回报，而过多的开发则意味着错过获得更高回报的机会。
​		更通俗的讲就是，一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益。
​		在现实生活和商业中，我们都面临着这种两难境地，没有正确的答案来教你如何做，可能是因为我们对世界的理解还不够清晰。然而，在数学领域，这个问题已经被研究过，被称为多臂赌博机问题 (Multi-armed bandit problem, K-armed bandit problem, MAB)，简称 MAB 问题，也被称为顺序资源分配问题。它被广泛用于广告推荐系统、源路由和棋盘游戏。

<img src="Final.assets/image-20210614182554821.png" alt="image-20210614182554821" style="zoom:25%;" />

1. 假设一个用户对不同类别的内容感兴趣程度不同，当推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这也是推荐系统常常面对的冷启动问题。

2. 假设系统中有若干广告库存物料，该给每个用户展示哪个广告，才能获得最大的点击收益，是不是每次都挑收益最好那个呢？

3. 算法工程师又设计出了新的策略或者模型，如何既能知道它和旧模型相比谁更靠谱又对风险可控呢？

​        这些问题全都是关于选择的问题。只要是关于选择的问题，都可以简化成一个 MAB 问题。

#### 2.2 Bandit算法与推荐系统

​		在推荐系统领域里，有两个比较经典的问题常被人提起，一个是EE问题，另一个是用户冷启动问题。
​		什么是EE问题？又叫exploit－explore问题。exploit就是：对用户比较确定的兴趣，当然要利用开采迎合，好比说已经挣到的钱，当然要花；explore就是：光对着用户已知的兴趣使用，用户很快会腻，所以要不断探索用户新的兴趣才行，这就好比虽然有一点钱可以花了，但是还得继续搬砖挣钱，不然花完了就得喝西北风。
用户冷启动问题，也就是面对新用户时，如何能够通过若干次实验，猜出用户的大致兴趣。
​		EE问题涉及到平衡准确和多样，而冷启动问题涉及到产品算法运营等一系列。Bandit算法是一种简单的在线学习算法，常常用于尝试解决这两个问题。
​		这两个问题本质上都是如何选择用户感兴趣的主题进行推荐，比较符合Bandit算法背后的MAB问题。



比如，用Bandit算法解决冷启动的大致思路如下：用分类或者Topic来表示每个用户兴趣，也就是MAB问题中的臂（Arm），我们可以通过几次试验，来刻画出新用户心目中对每个Topic的感兴趣概率。这里，如果用户对某个Topic感兴趣（提供了显式反馈或隐式反馈），就表示我们得到了收益，如果推给了它不感兴趣的Topic，推荐系统就表示很遗憾（regret）了。如此经历“选择-观察-更新-选择”的循环，理论上是越来越逼近用户真正感兴趣的Topic的。
		此外，我们要明白Bandit 算法并不是指一个算法，而是一类算法！

#### 2.3 Bandit算法的选择

​		现在来介绍一下Bandit算法怎么解决这类问题的。Bandit算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？
​		怎么衡量不同Bandit算法在解决多臂问题上的效果？首先介绍一个概念，叫做累积遗憾（regret）：

<img src="Final.assets/截屏2021-05-28 上午10.43.36.png" alt="截屏2021-05-28 上午10.43.36" style="zoom:50%;" />

这个公式就是计算Bandit算法的累积遗憾，解释一下：
		首先，这里我们讨论的每个臂的收益非0即1，也就是伯努利收益。
		然后，每次选择后，计算和最佳的选择差了多少，然后把差距累加起来就是总的遗憾。
		wB(i)是第i次试验时被选中臂的期望收益， w*是所有臂中的最佳那个，如果上帝提前告诉你，我们当然每次试验都选它，问题是上帝不告诉你，所以就有了Bandit算法。
		这个公式可以用来对比不同Bandit算法的效果：对同样的多臂问题，用不同的Bandit算法试验相同次数，看看谁的regret增长得慢。

​		那么到底不同的Bandit算法有哪些呢？

#### 2.4 常用Bandit算法

##### （1） Thompson sampling算法

Thompson sampling算法简单实用，因为它只有一行代码就可以实现[3]。简单介绍一下它的原理，要点如下：

1. 假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p。
2. 我们不断地试验，去估计出一个置信度较高的“概率p的概率分布”就能近似解决这个问题了。
3. 怎么能估计“概率p的概率分布”呢？ 答案是假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose。
4. 每个臂都维护一个beta分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。
5. 每次选择臂的方式是用每个臂现有的beta分布产生一个随机数b，选择所有臂产生的随机数中最大的那个臂去摇。

```python
import  numpy as np
import  pymc
#wins 和 trials 是一个N维向量，N是赌博机的臂的个数，每个元素记录了
choice = np.argmax(pymc.rbeta(1 + wins, 1 + trials - wins)) 
wins[choice] += 1
trials += 1
```

##### （2） UCB算法

UCB算法全称是Upper Confidence Bound（置信区间上界），它的算法步骤如下[4]：

- 初始化：先对每一个臂都试一遍；
- 按照如下公式计算每个臂的分数，然后选择分数最大的臂作为选择：

<img src="Final.assets/截屏2021-05-28 上午10.45.20.png" alt="截屏2021-05-28 上午10.45.20" style="zoom:33%;" />

- 观察选择结果，更新t和Tjt。其中加号前面是这个臂到目前的收益均值，后面的叫做bonus，本质上是均值的标准差，t是目前的试验次数，Tjt是这个臂被试次数。

这个公式反映一个特点：均值越大，标准差越小，被选中的概率会越来越大，同时哪些被选次数较少的臂也会得到试验机会。

##### （3） Epsilon-Greedy算法

这是一个朴素的Bandit算法，有点类似模拟退火的思想：

1. 选一个（0,1）之间较小的数作为epsilon；
2. 每次以概率epsilon做一件事：所有臂中随机选一个；
3. 每次以概率1-epsilon 选择截止到当前，平均收益最大的那个臂。

是不是简单粗暴？epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只想花钱不想挣钱。

##### （4） 朴素Bandit算法

​		最朴素的Bandit算法就是：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。这个算法是人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。

​		以上五个算法，经过我们多方查阅资料及实验，得到性能对比结果图如下

<img src="Final.assets/截屏2021-05-28 上午10.47.19.png" alt="截屏2021-05-28 上午10.47.19" style="zoom: 50%;" />

算法效果对比一目了然：UCB算法和Thompson采样算法显著优秀一些，所以我们本次实验决定使用并改进UCB算法。

#### 2.5 Bandit算法与线性回归——UCB

##### （1） UCB算法

​		UCB算法在做EE（Exploit-Explore）的时候表现不错，但它是上下文无关（context free）的Bandit算法，它只管埋头干活，根本不观察一下面对的都是些什么特点的arm，下次遇到相似特点但不一样的arm也帮不上什么忙。
​		UCB解决Multi-armed bandit问题的思路是：用置信区间。置信区间可以简单地理解为不确定性的程度，区间越宽，越不确定，反之亦反之。
​		每个item的回报均值都有个置信区间，随着试验次数增加，置信区间会变窄（逐渐确定了到底回报丰厚还是可怜）。每次选择前，都根据已经试验的结果重新估计每个Item的均值及置信区间。 选择置信区间上限最大的那个Item。
​		“选择置信区间上界最大的那个Item”这句话反映了几个意思：

1. 如果Item置信区间很宽（被选次数很少，还不确定），那么它会倾向于被多次选择，这个是算法冒风险的部分；
2. 如果Item置信区间很窄（备选次数很多，比较确定其好坏了），那么均值大的倾向于被多次选择，这个是算法保守稳妥的部分；
3. UCB是一种乐观的算法，选择置信区间上界排序，如果时悲观保守的做法，是选择置信区间下界排序。

##### （2） UCB算法加入特征信息

​		Yahoo!的科学家们在2010年发表了一篇论文[6]，给UCB引入了特征信息，同时还把改造后的UCB算法用在了Yahoo!的新闻推荐中，算法名叫LinUCB，刘鹏博士在《[计算广告](http://lib.csdn.net/base/计算广告)》一书中也有介绍LinUCB在计算广告中的应用。
​		单纯的老虎机回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由User和Item一起决定的，如果我们能用Feature来刻画User和Item这一对CP，在每次选择Item之前，通过Feature预估每一个arm（item）的期望回报及置信区间，选择的收益就可以通过Feature泛化到不同的Item上。
​		为UCB算法插上了特征的翅膀，这就是LinUCB最大的特色。
​		LinUCB算法做了一个假设：一个Item被选择后推送给一个User，其回报和相关Feature成线性关系，这里的“相关Feature”就是context，也是实际项目中发挥空间最大的部分。
​		于是试验过程就变成：用User和Item的特征预估回报及其置信区间，选择置信区间上界最大的Item推荐，观察回报后更新线性关系的参数，以此达到试验学习的目的。
​		LinUCB基本算法描述如下：

<img src="Final.assets/截屏2021-05-28 上午10.52.07.png" alt="截屏2021-05-28 上午10.52.07" style="zoom: 50%;" />

对照每一行解释一下（编号从1开始）：

1. 设定一个参数\alpha，这个参数决定了我们Explore的程度；
2. 开始试验迭代；
3. 获取每一个arm的特征向量xa,t；
4. 开始计算每一个arm的预估回报及其置信区间；
5. 如果arm还从没有被试验过，那么：
6. 用单位矩阵初始化Aa；
7. 用0向量初始化ba；
8. 处理完没被试验过的arm；
9. 计算线性参数\theta；
10. 用\theta和特征向量xa,t计算预估回报，同时加上置信区间宽度；
11. 处理完每一个arm；
12. 选择第10步中最大值对应的arm，观察真实的回报rt；
13. 更新Aat；
14. 更新bat；
15. 算法结束。

注意到上面的第4步，给特征矩阵加了一个单位矩阵，这就是岭回归（ridge regression），岭回归主要用于当样本数小于特征数时，对回归参数进行修正［8］。
对于加了特征的Bandit问题，正符合这个特点：试验次数（样本）少于特征数。
每一次观察真实回报之后，要更新的不止是岭回归参数，还有每个arm的回报向量ba。

##### （3）详解LinUCB的实现

根据论文给出的算法描述，其实很好写出LinUCB的代码，麻烦的只是构建特征。
代码如下，一些必要的注释说明已经写在代码中。

```python
class LinUCB:
    def __init__(self):
     self.alpha = 0.25 
     self.r1 = 1 # if worse -> 0.7, 0.8
        self.r0 = 0 # if worse, -19, -21
        # dimension of user features = d
        self.d = 6
        # Aa : collection of matrix to compute disjoint part for each article a, d*d
        self.Aa = {}
        # AaI : store the inverse of all Aa matrix
        self.AaI = {}
        # ba : collection of vectors to compute disjoin part, d*1
        self.ba = {}
 
        self.a_max = 0
 
        self.theta = {}
 
        self.x = None
        self.xT = None
        # linUCB
 
    def set_articles(self, art):
        # init collection of matrix/vector Aa, Ba, ba
        for key in art:
            self.Aa[key] = np.identity(self.d)
            self.ba[key] = np.zeros((self.d, 1))
            self.AaI[key] = np.identity(self.d)
            self.theta[key] = np.zeros((self.d, 1))
        # 这里更新参数时没有传入更新哪个arm，因为在上一次recommend的时候缓存了被选的那个arm，所以此处不用传入 
        # 另外，update操作不用阻塞recommend，可以异步执行        
    def update(self, reward):
        if reward == -1:
            pass
        elif reward == 1 or reward == 0:
            if reward == 1:
                r = self.r1
            else:
                r = self.r0
            self.Aa[self.a_max] += np.dot(self.x, self.xT)
            self.ba[self.a_max] += r * self.x
            self.AaI[self.a_max] = linalg.solve(self.Aa[self.a_max], np.identity(self.d))
            self.theta[self.a_max] = np.dot(self.AaI[self.a_max], self.ba[self.a_max])
        else:
        # error
            pass
        # 预估每个arm的回报期望及置信区间
    def recommend(self, timestamp, user_features, articles):
        xaT = np.array([user_features])
        xa = np.transpose(xaT)
        art_max = -1
        old_pa = 0
 
        # 获取在update阶段已经更新过的AaI(求逆结果)
        AaI_tmp = np.array([self.AaI[article] for article in articles])
        theta_tmp = np.array([self.theta[article] for article in articles])
        art_max = articles[np.argmax(np.dot(xaT, theta_tmp) + self.alpha * np.sqrt(np.dot(np.dot(xaT, AaI_tmp), xa)))]
 
        # 缓存选择结果，用于update
        self.x = xa
        self.xT = xaT
        # article index with largest UCB
        self.a_max = art_max
 
        return self.a_max
```

##### （4） 怎么构建特征

​		LinUCB算法有一个很重要的步骤，就是给User和Item构建特征，也就是刻画context。在原始论文里，Item是文章，其中专门介绍了它们怎么构建特征的，也甚是精妙。

原始用户特征

人口统计学：性别特征（2类），年龄特征（离散成10个区间）。

地域信息：遍布全球的大都市，美国各个州。

行为类别：代表用户历史行为的1000个类别取值。

原始文章特征

URL类别：根据文章来源分成了几十个类别。

编辑打标签：编辑人工给内容从几十个话题标签中挑选出来的原始特征向量都要归一化成单位向量。

还要对原始特征降维，以及模型要能刻画一些非线性的关系。

用Logistic Regression去拟合用户对文章的点击历史，其中的线性回归部分为： <img src="Final.assets/截屏2021-05-28 上午10.53.40.png" alt="截屏2021-05-28 上午10.53.40" style="zoom:50%;" />

拟合得到参数矩阵W，可以将原始用户特征（1000多维）投射到文章的原始特征空间（80多维），投射计算方式： <img src="Final.assets/截屏2021-05-28 上午10.53.56.png" alt="截屏2021-05-28 上午10.53.56" style="zoom:50%;" />

这是第一次降维，把原始1000多维降到80多维。
		然后，用投射后的80多维特征对用户聚类，得到5个类簇，文章页同样聚类成5个簇，再加上常数1，用户和文章各自被表示成6维向量。
		Yahoo!的科学家们之所以选定为6维，因为数据表明它的效果最好[10]，并且这大大降低了计算复杂度和存储空间。
		我们实际上可以考虑三类特征：U（用户），A（广告或文章），C（所在页面的一些信息）。
		前面说了，特征构建很有发挥空间，算法工程师们尽情去挥洒汗水吧。
总结一下LinUCB算法，有以下优点：

1. 由于加入了特征，所以收敛比UCB更快；
2. 特征构建是效果的关键，也是工程上最麻烦和值的发挥的地方；
3. 由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章；
4. 特征降维很有必要，关系到计算效率。



#### 2.6 总结

​		Exploit-Explore这一对矛盾一直客观存在，Bandit算法是公认的一种比较好的解决EE问题的方案。除了Bandit算法之外，还有一些其他的explore的办法，比如：在推荐时，随机地去掉一些用户历史行为（特征）。
​		解决Explore，势必就是要冒险，势必要走向未知，而这显然就是会伤害用户体验的：明知道用户肯定喜欢A，你还偏偏以某个小概率给推荐非A。
​		实际上，很少有公司会采用这些理性的办法做Explore，反而更愿意用一些盲目主观的方式。究其原因，可能是因为：

1. 互联网产品生命周期短，而Explore又是为了提升长期利益的，所以没有动力做；
2. 用户使用互联网产品时间越来越碎片化，Explore的时间长，难以体现出Explore 的价值；
3. 同质化互联网产品多，用户选择多，稍有不慎，用户用脚投票，分分钟弃你于不顾；
4. 已经成规模的平台，红利杠杠的，其实是没有动力做Explore的。

基于这些，我们如果想在自己的推荐系统中引入Explore机制，需要注意以下几点：

1. 用于Explore的Item要保证其本身质量，纵使用户不感兴趣，也不至于引起其反感；

2. Explore本身的产品需要精心设计，让用户有耐心陪你玩儿；

3. 深度思考，这样才不会做出脑残的产品，产品不会早早夭折，才有可能让Explore机制有用武之地。

   

<div style="page-break-after:always"></div>

## 3、

<div style="page-break-after:always"></div>

## 4、



## 5、提交文件列表





